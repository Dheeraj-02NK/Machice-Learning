{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04974d0",
   "metadata": {},
   "source": [
    "### Demonstrate Q learning algorithm with suitable assumption for a problem statement\n",
    "### Assumed problem statement : a 2D grid world where an agent needs to find the shortest path to a goal while avoiding obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2d588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Path: [(0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (2, 3), (2, 4), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment and Q-table\n",
    "# 0: empty cell, 1: obstacle, 2: goal\n",
    "env = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 1, 0], [0, 1, 0, 1, 0], [0, 0, 0, 1, 2]])\n",
    "Q = np.zeros((np.prod(env.shape), 4))\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.8\n",
    "dr = 0.95\n",
    "ep = 0.2\n",
    "ne = 1000\n",
    "\n",
    "# Define A\n",
    "A = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "# Perform Q-learning\n",
    "for _ in range(ne):\n",
    "    S = (0, 0)\n",
    "    while env[S] != 2:\n",
    "        action = np.random.choice(4) if np.random.uniform(0, 1) < ep else np.argmax(Q[S[0]*env.shape[1]+S[1]])\n",
    "        new_S = (max(min(S[0]+A[action][0], env.shape[0]-1), 0), max(min(S[1]+A[action][1], env.shape[1]-1), 0))\n",
    "        reward = -1 if env[new_S] == 1 else (10 if env[new_S] == 2 else 0)\n",
    "        Q[S[0]*env.shape[1]+S[1]][action] += lr * (reward + dr*np.max(Q[new_S[0]*env.shape[1]+new_S[1]]) - \n",
    "                                 Q[S[0]*env.shape[1]+S[1]][action])\n",
    "        S = new_S\n",
    "\n",
    "# Find the optimal path\n",
    "S = (0, 0)\n",
    "optimal_path = [S]\n",
    "while env[S] != 2:\n",
    "    action = np.argmax(Q[S[0]*env.shape[1]+S[1]])\n",
    "    S = (max(min(S[0]+A[action][0], env.shape[0]-1), 0), max(min(S[1]+A[action][1], env.shape[1]-1), 0))\n",
    "    optimal_path.append(S)\n",
    "\n",
    "print(\"Optimal Path:\", optimal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf6c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Path:\n",
      "[0 0 0 1 0]\n",
      "[0 1 0 1 0]\n",
      "[0 1 0 1 0]\n",
      "[0 0 0 1 2]\n",
      "Q-table:\n",
      "[[ 6.24828107  5.93586702  6.24828107  6.57713797]\n",
      " [ 6.57713797  5.92330312  6.24828107  6.92330312]\n",
      " [ 6.92330312  7.2876875   6.57713797  7.1450625 ]\n",
      " [ 7.13148793  7.56566689  6.87816032  8.57375   ]\n",
      " [ 8.57371218  9.025       7.1450625   8.57361483]\n",
      " [ 6.24828107  5.62599421  5.88967384  5.89941801]\n",
      " [ 6.56899291  6.13529146  5.89717466  7.2876875 ]\n",
      " [ 6.92330312  7.67125     5.92330312  7.57375   ]\n",
      " [ 7.14506167  8.025       7.2424865   9.025     ]\n",
      " [ 8.57375     9.5         7.57374991  9.025     ]\n",
      " [ 5.93580696  0.          0.          4.22894737]\n",
      " [ 5.83821894  7.67125     4.44743819  6.099     ]\n",
      " [ 7.2876875   8.075       6.28768744  8.025     ]\n",
      " [ 7.57375     8.5         7.671174    9.5       ]\n",
      " [ 9.025      10.          8.025       9.5       ]\n",
      " [ 5.61390044  0.          0.          0.        ]\n",
      " [ 6.25718834  7.60988     5.063134    8.075     ]\n",
      " [ 7.67125     8.075       7.67125     8.5       ]\n",
      " [ 8.025       8.5         8.075      10.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Optimal Path:\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "(3, 3)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment (2D grid world)\n",
    "# 0: empty cell, 1: obstacle, 2: goal\n",
    "env = np.array([\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 1, 2]\n",
    "])\n",
    "\n",
    "# Define Q-table (state-action values)\n",
    "num_states = np.prod(env.shape)\n",
    "num_actions = 4  # Up, Down, Left, Right\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "exploration_prob = 0.2\n",
    "num_episodes = 1000\n",
    "\n",
    "# Convert 2D grid to 1D state representation\n",
    "def state_to_index(state):\n",
    "    return state[0] * env.shape[1] + state[1]\n",
    "\n",
    "# Perform Q-learning\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Start from the top-left corner\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < exploration_prob:\n",
    "            action = np.random.choice(num_actions)  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(Q[state_to_index(state)])  # Exploitation\n",
    "\n",
    "        new_state = state\n",
    "        if action == 0:  # Up\n",
    "            new_state = (max(state[0] - 1, 0), state[1])\n",
    "        elif action == 1:  # Down\n",
    "            new_state = (min(state[0] + 1, env.shape[0] - 1), state[1])\n",
    "        elif action == 2:  # Left\n",
    "            new_state = (state[0], max(state[1] - 1, 0))\n",
    "        elif action == 3:  # Right\n",
    "            new_state = (state[0], min(state[1] + 1, env.shape[1] - 1))\n",
    "\n",
    "        if env[new_state] == 1:\n",
    "            reward = -1  # Penalty for hitting an obstacle\n",
    "        elif env[new_state] == 2:\n",
    "            reward = 10  # Reward for reaching the goal\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0  # No immediate reward for other states\n",
    "\n",
    "        Q[state_to_index(state)][action] = Q[state_to_index(state)][action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(Q[state_to_index(new_state)]) - Q[state_to_index(state)][action])\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "# Find the optimal path\n",
    "state = (0, 0)\n",
    "optimal_path = [state]\n",
    "while state != (3, 4):  # Goal state\n",
    "    action = np.argmax(Q[state_to_index(state)])\n",
    "    if action == 0:\n",
    "        state = (max(state[0] - 1, 0), state[1])\n",
    "    elif action == 1:\n",
    "        state = (min(state[0] + 1, env.shape[0] - 1), state[1])\n",
    "    elif action == 2:\n",
    "        state = (state[0], max(state[1] - 1, 0))\n",
    "    elif action == 3:\n",
    "        state = (state[0], min(state[1] + 1, env.shape[1] - 1))\n",
    "    optimal_path.append(state)\n",
    "\n",
    "print(\"Optimal Path:\")\n",
    "for row in env:\n",
    "    print(row)\n",
    "\n",
    "print(\"Q-table:\")\n",
    "print(Q)\n",
    "\n",
    "print(\"Optimal Path:\")\n",
    "for state in optimal_path:\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dc37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
